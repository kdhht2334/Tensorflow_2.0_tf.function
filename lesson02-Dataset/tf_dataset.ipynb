{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 2.0 Dataset API using MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Tensorflow dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, datasets, Sequential, metrics\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Install tensorflow-datasets and apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow like below:\n",
    "# ~$ pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/3 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading / extracting dataset mnist (11.06 MiB) to /Users/daehakim/tensorflow_datasets/mnist/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...:   0%|          | 0/9 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  1.59 url/s]\n",
      "Dl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  1.59 url/s]\n",
      "Dl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...:   0%|          | 0/1 [00:00<?, ? file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  1.59 url/s]6 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:01,  1.91 url/s]6 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:01,  1.91 url/s]6 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...:  50%|█████     | 1/2 [00:00<00:00,  1.56 file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:01,  1.91 url/s]7 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|          | 0/10 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 2/2 [00:00<00:00,  1.87 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:07<00:01,  1.91 url/s]\n",
      "Dl Size...:  10%|█         | 1/10 [00:07<01:06,  7.36s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 2/2 [00:07<00:00,  1.87 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:12<00:01,  1.91 url/s]\n",
      "Dl Size...:  20%|██        | 2/10 [00:12<00:52,  6.62s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 2/2 [00:12<00:00,  1.87 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:16<00:01,  1.91 url/s]\n",
      "Dl Size...:  30%|███       | 3/10 [00:16<00:40,  5.83s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:17<00:05,  5.25s/ url]7 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:  30%|███       | 3/10 [00:17<00:40,  5.83s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:17<00:05,  5.25s/ url]7 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:  30%|███       | 3/10 [00:17<00:40,  5.83s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...:  67%|██████▋   | 2/3 [00:17<00:00,  1.87 file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:17<00:05,  5.25s/ url]0s/ file]\u001b[A\u001b[A\n",
      "Dl Size...:  30%|███       | 3/10 [00:17<00:40,  5.83s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [00:17<00:00,  5.30s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:22<00:05,  5.25s/ url]\n",
      "Dl Size...:  40%|████      | 4/10 [00:22<00:35,  5.84s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [00:22<00:00,  5.30s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:26<00:05,  5.25s/ url]\n",
      "Dl Size...:  50%|█████     | 5/10 [00:26<00:26,  5.38s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [00:26<00:00,  5.30s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:31<00:05,  5.25s/ url]\n",
      "Dl Size...:  60%|██████    | 6/10 [00:31<00:20,  5.14s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [00:31<00:00,  5.30s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:35<00:05,  5.25s/ url]\n",
      "Dl Size...:  70%|███████   | 7/10 [00:35<00:14,  4.89s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [00:35<00:00,  5.30s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:40<00:05,  5.25s/ url]\n",
      "Dl Size...:  80%|████████  | 8/10 [00:40<00:09,  4.99s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [00:40<00:00,  5.30s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:50<00:05,  5.25s/ url]\n",
      "Dl Size...:  90%|█████████ | 9/10 [00:50<00:06,  6.35s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [00:50<00:00,  5.30s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:54<00:05,  5.25s/ url]\n",
      "Dl Size...: 100%|██████████| 10/10 [00:54<00:00,  5.83s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:57<00:00, 15.66s/ url]0s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 10/10 [00:57<00:00,  5.83s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:57<00:00, 15.66s/ url]0s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 10/10 [00:57<00:00,  5.83s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...:  75%|███████▌  | 3/4 [00:57<00:05,  5.30s/ file]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:57<00:00, 15.66s/ url]1s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 10/10 [00:57<00:00,  5.83s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 4/4 [00:57<00:00, 15.81s/ file]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "0 examples [00:00, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000 examples [00:15, 3997.77 examples/s]\n",
      "Shuffling...:   0%|          | 0/10 [00:00<?, ? shard/s]WARNING: Logging before flag parsing goes to stderr.\n",
      "W0311 20:24:55.365458 4675810752 deprecation.py:323] From /anaconda3/envs/tensorflow-2.0/lib/python3.6/site-packages/tensorflow_datasets/core/file_format_adapter.py:249: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 216802.85 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Writing...: 100%|██████████| 6000/6000 [00:00<00:00, 186497.78 examples/s]\u001b[A\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 325505.72 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Shuffling...:  20%|██        | 2/10 [00:00<00:00, 15.27 shard/s]xamples/s]\u001b[A\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 320236.99 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Writing...: 100%|██████████| 6000/6000 [00:00<00:00, 179684.01 examples/s]\u001b[A\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 311319.51 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Shuffling...:  40%|████      | 4/10 [00:00<00:00, 15.36 shard/s]xamples/s]\u001b[A\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 301296.91 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Writing...: 100%|██████████| 6000/6000 [00:00<00:00, 179470.02 examples/s]\u001b[A\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 303597.74 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Shuffling...:  60%|██████    | 6/10 [00:00<00:00, 15.56 shard/s]xamples/s]\u001b[A\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 313757.03 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Writing...: 100%|██████████| 6000/6000 [00:00<00:00, 178421.55 examples/s]\u001b[A\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 323484.81 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Shuffling...:  80%|████████  | 8/10 [00:00<00:00, 15.65 shard/s]xamples/s]\u001b[A\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 315258.49 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Writing...: 100%|██████████| 6000/6000 [00:00<00:00, 177064.51 examples/s]\u001b[A\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 6000 examples [00:00, 312285.31 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/6000 [00:00<?, ? examples/s]\u001b[A\n",
      "Shuffling...: 100%|██████████| 10/10 [00:00<00:00, 15.76 shard/s]amples/s]\u001b[A\n",
      "10000 examples [00:02, 3361.70 examples/s]\n",
      "Shuffling...:   0%|          | 0/1 [00:00<?, ? shard/s]\n",
      "Reading...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Reading...: 10000 examples [00:00, 329014.05 examples/s]\u001b[A\n",
      "Writing...:   0%|          | 0/10000 [00:00<?, ? examples/s]\u001b[A\n",
      "Shuffling...: 100%|██████████| 1/1 [00:00<00:00, 10.65 shard/s]4 examples/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "train_ds = tfds.load(\"mnist\",\n",
    "                               split=\"train\",\n",
    "                     as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops._OptionsDataset"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.shuffle(1024).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  200960    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  2570      \n",
      "=================================================================\n",
      "Total params: 335,114\n",
      "Trainable params: 335,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build simle neural network using keras Sequential API\n",
    "network = Sequential([layers.Dense(256, activation='relu'),\n",
    "                     layers.Dense(256, activation='relu'),\n",
    "                     layers.Dense(256, activation='relu'),\n",
    "                     layers.Dense(10)])\n",
    "network.build(input_shape=(None, 28*28))\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimizer\n",
    "optimizer = optimizers.SGD(lr=0.01)\n",
    "acc_meter = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7a4109742467>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/tensorflow-2.0/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     return distribute_ctx.get_replica_context().merge_call(\n\u001b[0;32m--> 406\u001b[0;31m         self._distributed_apply, args=(grads_and_vars,), kwargs={\"name\": name})\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_distributed_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow-2.0/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow-2.0/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[1;32m   1381\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow-2.0/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[0;34m(self, distribution, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    438\u001b[0m           update_ops.extend(\n\u001b[1;32m    439\u001b[0m               distribution.extended.update(\n\u001b[0;32m--> 440\u001b[0;31m                   var, apply_grad_to_update_var, args=(grad,), group=False))\n\u001b[0m\u001b[1;32m    441\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mapply_updates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow-2.0/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow-2.0/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   1544\u001b[0m     \u001b[0;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m     \u001b[0;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow-2.0/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0;31m# once that value is used for something.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow-2.0/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m    423\u001b[0m         return self._resource_apply_sparse_duplicate_indices(\n\u001b[1;32m    424\u001b[0m             grad.values, var, grad.indices)\n\u001b[0;32m--> 425\u001b[0;31m       \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource_apply_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow-2.0/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/gradient_descent.py\u001b[0m in \u001b[0;36m_resource_apply_dense\u001b[0;34m(self, grad, var)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m       return training_ops.resource_apply_gradient_descent(\n\u001b[0;32m--> 116\u001b[0;31m           var.handle, lr_t, grad, use_locking=self._use_locking)\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_resource_apply_sparse_duplicate_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow-2.0/lib/python3.6/site-packages/tensorflow/python/training/gen_training_ops.py\u001b[0m in \u001b[0;36mresource_apply_gradient_descent\u001b[0;34m(var, alpha, delta, use_locking, name)\u001b[0m\n\u001b[1;32m   1944\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m         \u001b[0;34m\"ResourceApplyGradientDescent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1946\u001b[0;31m         var, alpha, delta, \"use_locking\", use_locking)\n\u001b[0m\u001b[1;32m   1947\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train like this\n",
    "for (x, y) in train_ds.repeat(10):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        x = tf.reshape(x, (-1, 28*28))\n",
    "        out = network(x)\n",
    "        \n",
    "        y_onehot = tf.one_hot(y, depth=10)\n",
    "        \n",
    "        loss = tf.square(out - y_onehot)\n",
    "        loss = tf.reduce_sum(loss) / 32\n",
    "        \n",
    "    acc_meter.update_state(tf.argmax(out, axis=1), y)\n",
    "    \n",
    "    grads = tape.gradient(loss, network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, network.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It can be possible to load imagenet2012 dataset, directly!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ds = tfds.load(\"imagenet2012\",\n",
    "#                               split=\"train\",\n",
    "#                               as_supervised=True)\n",
    "#train_ds = train_ds.shuffle(1024).batch(32)\n",
    "\n",
    "#for (x, y) in train_ds.repeat(10):\n",
    "#   ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Convert simple numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tfds.load(\"mnist\", split=\"train\", batch_size=-1)\n",
    "numpy_ds = tfds.as_numpy(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, lbs = numpy_ds[\"image\"], numpy_ds[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of image is (60000, 28, 28, 1)\n",
      "shape of label is (60000,)\n"
     ]
    }
   ],
   "source": [
    "print('shape of image is {}'.format(np.shape(imgs)))\n",
    "print('shape of label is {}'.format(np.shape(lbs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb2fccc3c8>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs = np.squeeze(imgs, axis=3)\n",
    "plt.imshow(imgs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we can train simple neural network using numpy MNIST datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build neural network using keras custom model\n",
    "class MyModel(keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.layer1 = layers.Dense(128, activation=tf.nn.relu)\n",
    "        self.layer2 = layers.Dense(64, activation=tf.nn.relu)\n",
    "        self.layer3 = layers.Dense(10, activation=tf.nn.sigmoid)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             multiple                  100480    \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             multiple                  650       \n",
      "=================================================================\n",
      "Total params: 109,386\n",
      "Trainable params: 109,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 150s 301ms/step - loss: 102.8920 - accuracy: 0.9993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb34d5ab70>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel()\n",
    "model.build(input_shape=(None, 28*28))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(1e-1), loss='categorical_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "model.fit(imgs, lbs, epochs=1, steps_per_epoch=500, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
